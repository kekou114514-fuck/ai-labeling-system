import os
import sys
import torch
import evaluate
import numpy as np
import librosa  
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

sys.stdout.reconfigure(line_buffering=True)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR = os.path.join(BASE_DIR, "dataset")
OUTPUT_DIR = os.path.join(BASE_DIR, "whisper-finetuned-model")
MODEL_PATH = "/app/models/whisper"

# ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ˜ç¡®å¯¼å…¥ DatasetDict
import datasets 
from datasets import load_dataset, DatasetDict 

from transformers import (
    WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, 
    Seq2SeqTrainingArguments, Seq2SeqTrainer
)

def main():
    if not os.path.exists(os.path.join(DATASET_DIR, "metadata.csv")):
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° dataset/metadata.csv")
        sys.exit(1)

    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    try:
        processor = WhisperProcessor.from_pretrained(MODEL_PATH, language="Chinese", task="transcribe")
        tokenizer = WhisperTokenizer.from_pretrained(MODEL_PATH, language="Chinese", task="transcribe")
        model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []

    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset("csv", data_files=os.path.join(DATASET_DIR, "metadata.csv"), split="train")
    
    # 1. æŠŠç›¸å¯¹è·¯å¾„è½¬ä¸ºç»å¯¹è·¯å¾„
    def resolve_audio_path(batch):
        batch["audio_path"] = [os.path.join(DATASET_DIR, f) for f in batch["file_name"]]
        return batch
    
    dataset = dataset.map(resolve_audio_path, batched=True)

    # 2. æ‰‹åŠ¨è¯»å–éŸ³é¢‘ (librosa)
    def prepare_dataset(batch):
        path = batch["audio_path"]
        try:
            # å¼ºåˆ¶é‡é‡‡æ ·åˆ° 16k
            speech_array, sampling_rate = librosa.load(path, sr=16000)
            
            # æå–ç‰¹å¾
            batch["input_features"] = processor.feature_extractor(
                speech_array, sampling_rate=sampling_rate
            ).input_features[0]
            
            # ç¼–ç æ ‡ç­¾
            batch["labels"] = tokenizer(batch["sentence"]).input_ids
        except Exception as e:
            print(f"âš ï¸ è¯»å–éŸ³é¢‘å¤±è´¥ {path}: {e}")
            batch["input_features"] = [] 
            batch["labels"] = []
            
        return batch

    print("ğŸ“Š é¢„å¤„ç†æ•°æ® (ä½¿ç”¨ librosa æ‰‹åŠ¨è¯»å–)...")
    dataset = dataset.map(prepare_dataset, num_proc=1).filter(lambda x: len(x["input_features"]) > 0)
    
    # åˆ’åˆ†éªŒè¯é›†
    if len(dataset) > 5:
        dataset = dataset.train_test_split(test_size=0.1)
    else:
        print("âš ï¸ æ•°æ®é‡è¾ƒå°‘ï¼Œè·³è¿‡éªŒè¯é›†åˆ’åˆ†")
        # ğŸ”¥ ä¿®å¤ç‚¹ï¼šç°åœ¨ DatasetDict å·²ç»å¯¼å…¥äº†ï¼Œä¸ä¼šæŠ¥é”™äº†
        dataset = DatasetDict({"train": dataset, "test": dataset})

    @dataclass
    class DataCollatorSpeechSeq2SeqWithPadding:
        processor: Any
        def __call__(self, features):
            input_features = [{"input_features": feature["input_features"]} for feature in features]
            batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
            label_features = [{"input_ids": feature["labels"]} for feature in features]
            labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
            labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
            if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
                labels = labels[:, 1:]
            batch["labels"] = labels
            return batch

    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    metric = evaluate.load("wer")

    def compute_metrics(pred):
        pred_ids = pred.predictions
        label_ids = pred.label_ids
        label_ids[label_ids == -100] = tokenizer.pad_token_id
        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
        wer = 100 * metric.compute(predictions=pred_str, references=label_str)
        return {"wer": wer}

    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=2, 
        gradient_accumulation_steps=1,
        learning_rate=1e-5,
        max_steps=50, 
        fp16=torch.cuda.is_available(),
        logging_steps=5,
        save_steps=25,
        report_to=[], 
        remove_unused_columns=False 
    )

    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        tokenizer=processor.feature_extractor,
    )

    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    processor.save_pretrained(OUTPUT_DIR)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ–°æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
