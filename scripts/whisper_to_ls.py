import os
import glob
import json
import argparse
import torch
import librosa
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from tqdm import tqdm

# ==========================================
# âš™ï¸ Docker é€‚é…é…ç½®
# ==========================================
DATA_ROOT = os.getenv('DATA_ROOT', '/data')
LS_URL_PREFIX = "/data/local-files/?d=/data/"

# å¼ºåˆ¶ç¦»çº¿ï¼Œä¼˜å…ˆä½¿ç”¨ Docker å†…ç½®æ¨¡å‹
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

def run_inference(project_type):
    # === P2: çº¯éŸ³é¢‘ ===
    if project_type == '2':
        print("ğŸ§ æ¨¡å¼: é¡¹ç›® 2 (çº¯éŸ³é¢‘)")
        config = {
            "audio_dir": os.path.join(DATA_ROOT, "audio"),
            # ä¼˜å…ˆè¯»å–æ‚¨ç²˜è´´è¿›å»çš„ç¦»çº¿æ¨¡å‹
            "model_path": "/app/models/whisper", 
            "output": os.path.join(DATA_ROOT, "outputs/pre_annotations_audio.json")
        }
    # === P3: è§†é¢‘è¯­éŸ³ ===
    elif project_type == '3':
        print("ğŸ¬ æ¨¡å¼: é¡¹ç›® 3 (è§†é¢‘æå–éŸ³é¢‘)")
        config = {
            "audio_dir": os.path.join(DATA_ROOT, "video_audio"),
            # å¦‚æœ P3 æœ‰ä¸“é—¨å¾®è°ƒçš„æ¨¡å‹ï¼Œå¯ä»¥æ”¹è¿™é‡Œï¼›é»˜è®¤ä¹Ÿç”¨åŸºç¡€æ¨¡å‹
            "model_path": "/app/models/whisper",
            "output": os.path.join(DATA_ROOT, "outputs/pre_annotations_video_audio.json")
        }
    else:
        print("âŒ æœªçŸ¥é¡¹ç›®ç±»å‹")
        return

    # 1. æ£€æŸ¥éŸ³é¢‘ç›®å½•
    if not os.path.exists(config['audio_dir']):
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶å¤¹: {config['audio_dir']}")
        return

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸ§  åŠ è½½æ¨¡å‹: {config['model_path']}")
    try:
        if os.path.exists(os.path.join(config['model_path'], "config.json")):
            model = WhisperForConditionalGeneration.from_pretrained(config['model_path'])
            processor = WhisperProcessor.from_pretrained(config['model_path'])
        else:
            print("âš ï¸ ç¦»çº¿æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°è¯•è”ç½‘åŠ è½½ openai/whisper-small...")
            os.environ["HF_HUB_OFFLINE"] = "0" # ä¸´æ—¶å¼€å¯è”ç½‘
            model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
            processor = WhisperProcessor.from_pretrained("openai/whisper-small")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        print(f"ğŸš€ æ¨¡å‹å·²åŠ è½½è‡³ {device}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. æ‰«ææ–‡ä»¶
    extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a', '*.ogg']
    audio_files = []
    for ext in extensions:
        audio_files.extend(glob.glob(os.path.join(config['audio_dir'], ext)))
        audio_files.extend(glob.glob(os.path.join(config['audio_dir'], ext.upper())))

    if not audio_files:
        print(f"âŒ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {config['audio_dir']}")
        return

    print(f"ğŸ¤ å¼€å§‹å¤„ç† {len(audio_files)} ä¸ªæ–‡ä»¶...")
    results_list = []

    for audio_path in tqdm(audio_files):
        try:
            # è¯»å–å¹¶è½¬å†™
            speech, _ = librosa.load(audio_path, sr=16000)
            input_features = processor(speech, sampling_rate=16000, return_tensors="pt").input_features.to(device)
            
            with torch.no_grad():
                predicted_ids = model.generate(input_features, language="zh", task="transcribe")
            
            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

            # ç”Ÿæˆç›¸å¯¹è·¯å¾„ URL
            rel_path = os.path.relpath(audio_path, DATA_ROOT)
            ls_url = f"{LS_URL_PREFIX}{rel_path}"

            results_list.append({
                "data": {"audio": ls_url},
                "predictions": [{
                    "model_version": "whisper_v1",
                    "result": [{
                        "from_name": "transcription",
                        "to_name": "audio",
                        "type": "textarea",
                        "value": {"text": [transcription]}
                    }]
                }]
            })
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {os.path.basename(audio_path)}: {e}")

    # 4. ä¿å­˜
    os.makedirs(os.path.dirname(config['output']), exist_ok=True)
    with open(config['output'], 'w', encoding='utf-8') as f:
        json.dump(results_list, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç”Ÿæˆå®Œæ¯•: {config['output']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--project", type=str, required=True)
    args = parser.parse_args()
    run_inference(args.project)
