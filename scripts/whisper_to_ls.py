import os
import glob
import json
import argparse
import torch
import librosa
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from tqdm import tqdm

DATA_ROOT = os.getenv('DATA_ROOT', '/data')
LS_URL_PREFIX = "/data/local-files/?d=/data/"
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# å¹»è§‰è¯è¿‡æ»¤
HALLUCINATION_PHRASES = ["ä½ å¥½", "å­—å¹•", "è¯·è®¢é˜…", "è°¢è°¢è§‚çœ‹", "Subtitle", "Copyright"]

def run_inference(project_type):
    # === P2: çº¯éŸ³é¢‘ (é€šå¸¸æ˜¯ ID 3) ===
    if project_type in ['2', '3']:
        print(f"ğŸ§ æ¨¡å¼: é¡¹ç›® {project_type} (çº¯éŸ³é¢‘)")
        config = {
            "audio_dir": os.path.join(DATA_ROOT, "audio"),
            "model_path": "/app/models/whisper", 
            "output": os.path.join(DATA_ROOT, "outputs/pre_annotations_audio.json")
        }
    # === P3: è§†é¢‘è¯­éŸ³ (ID 5 æˆ– 6) ===
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ  ID 5 æ”¯æŒ
    elif project_type in ['5', '6']:
        print(f"ğŸ¬ æ¨¡å¼: é¡¹ç›® {project_type} (è§†é¢‘æå–éŸ³é¢‘)")
        config = {
            "audio_dir": os.path.join(DATA_ROOT, "video_audio"),
            # ä¼˜å…ˆä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
            "model_path": "/app/scripts/train_whisper_video/whisper-finetuned-model",
            "output": os.path.join(DATA_ROOT, "outputs/pre_annotations_video_audio.json")
        }
    else:
        print(f"âŒ æœªçŸ¥é¡¹ç›®ç±»å‹: {project_type}"); return

    # æ¨¡å‹å›é€€é€»è¾‘
    if not os.path.exists(config['model_path']):
        print(f"âš ï¸ å¾®è°ƒæ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: /app/models/whisper")
        config['model_path'] = "/app/models/whisper"

    # 1. æ‰«ææ–‡ä»¶
    if not os.path.exists(config['audio_dir']):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {config['audio_dir']}"); return

    extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']
    audio_files = []
    for ext in extensions:
        audio_files.extend(glob.glob(os.path.join(config['audio_dir'], ext)))
        audio_files.extend(glob.glob(os.path.join(config['audio_dir'], ext.upper())))

    if not audio_files:
        print(f"âŒ æœªæ‰¾åˆ°éŸ³é¢‘: {config['audio_dir']}"); return

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸ§  åŠ è½½æ¨¡å‹: {config['model_path']}")
    try:
        model = WhisperForConditionalGeneration.from_pretrained(config['model_path'])
        processor = WhisperProcessor.from_pretrained(config['model_path'])
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}"); return

    print(f"ğŸ¤ å¼€å§‹æ¨ç† {len(audio_files)} ä¸ªæ–‡ä»¶...")
    results_list = []

    for audio_path in tqdm(audio_files):
        try:
            speech, _ = librosa.load(audio_path, sr=16000)
            
            # é™éŸ³æ£€æµ‹
            if np.max(np.abs(speech)) < 0.005:
                transcription = "[é™éŸ³]"
            else:
                if len(speech) > 16000 * 30: speech = speech[:16000*30] # åªå–å‰30ç§’
                
                input_features = processor(speech, sampling_rate=16000, return_tensors="pt").input_features.to(device)
                with torch.no_grad():
                    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                    predicted_ids = model.generate(input_features, task="transcribe", no_repeat_ngram_size=2, num_beams=5)
                transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()

                # å¹»è§‰è¿‡æ»¤
                if transcription in HALLUCINATION_PHRASES or len(transcription) < 2:
                    transcription = f"[å¯èƒ½ä¸ºå™ªéŸ³] ({transcription})"

            rel_path = os.path.relpath(audio_path, DATA_ROOT)
            ls_url = f"{LS_URL_PREFIX}{rel_path}"

            # ğŸ”¥ é€‚é… P3 XML: to_name="audio", from_name="transcription", type="textarea"
            results_list.append({
                "data": {"audio": ls_url},
                "predictions": [{
                    "model_version": "v1",
                    "result": [{
                        "from_name": "transcription", 
                        "to_name": "audio", 
                        "type": "textarea",
                        "value": {"text": [transcription]}
                    }]
                }]
            })
        except: continue

    os.makedirs(os.path.dirname(config['output']), exist_ok=True)
    with open(config['output'], 'w', encoding='utf-8') as f:
        json.dump(results_list, f, indent=2, ensure_ascii=False)
    print(f"âœ… ç”Ÿæˆå®Œæ¯•: {config['output']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--project", type=str, required=True)
    args = parser.parse_args()
    run_inference(args.project)
