import os
import sys
import torch
import evaluate
import librosa
import numpy as np
from dataclasses import dataclass
from typing import Any, Dict, List, Union
import datasets
from datasets import load_dataset, DatasetDict

sys.stdout.reconfigure(line_buffering=True)
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR = os.path.join(BASE_DIR, "dataset")
OUTPUT_DIR = os.path.join(BASE_DIR, "whisper-finetuned-model")
MODEL_PATH = "/app/models/whisper"

from transformers import (
    WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, 
    Seq2SeqTrainingArguments, Seq2SeqTrainer
)

def main():
    if not os.path.exists(os.path.join(DATASET_DIR, "metadata.csv")):
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ã€‚")
        sys.exit(1)

    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    try:
        processor = WhisperProcessor.from_pretrained(MODEL_PATH, language="Chinese", task="transcribe")
        tokenizer = WhisperTokenizer.from_pretrained(MODEL_PATH, language="Chinese", task="transcribe")
        model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}"); sys.exit(1)

    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []

    # åŠ è½½æ•°æ®
    dataset = load_dataset("csv", data_files=os.path.join(DATASET_DIR, "metadata.csv"), split="train")
    total_samples = len(dataset)
    print(f"ğŸ“Š æ•°æ®é›†æ€»é‡: {total_samples} æ¡")

    def prepare_dataset(batch):
        path = os.path.join(DATASET_DIR, batch["file_name"])
        try:
            speech, _ = librosa.load(path, sr=16000)
            batch["input_features"] = processor.feature_extractor(speech, sampling_rate=16000).input_features[0]
            batch["labels"] = tokenizer(batch["sentence"]).input_ids
        except:
            batch["input_features"] = None
        return batch

    dataset = dataset.map(prepare_dataset, num_proc=1).filter(lambda x: x["input_features"] is not None)
    
    # ğŸ”¥ æ ¸å¿ƒä¿®æ­£ï¼šå•æ ·æœ¬/å°‘æ ·æœ¬ç­–ç•¥
    if len(dataset) < 2:
        print("âš ï¸ è­¦å‘Šï¼šæ ·æœ¬æå°‘ (<2)ï¼Œè·³è¿‡éªŒè¯é›†åˆ’åˆ†ï¼Œå¼€å¯å…¨é‡è¿‡æ‹Ÿåˆè®­ç»ƒæ¨¡å¼ã€‚")
        dataset = DatasetDict({"train": dataset, "test": dataset}) # testå³trainï¼Œä»…ä¸ºé˜²æŠ¥é”™
        eval_strategy = "no"
        save_steps = 10
        logging_steps = 1
    else:
        # æ­£å¸¸åˆ’åˆ†
        dataset = dataset.train_test_split(test_size=0.1)
        eval_strategy = "steps"
        save_steps = 50
        logging_steps = 10

    @dataclass
    class DataCollator:
        processor: Any
        def __call__(self, features):
            input_features = [{"input_features": f["input_features"]} for f in features]
            batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
            label_features = [{"input_ids": f["labels"]} for f in features]
            labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
            labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
            if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
                labels = labels[:, 1:]
            batch["labels"] = labels
            return batch

    metric = evaluate.load("wer")
    def compute_metrics(pred):
        pred_ids = pred.predictions
        label_ids = pred.label_ids
        label_ids[label_ids == -100] = tokenizer.pad_token_id
        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
        return {"wer": 100 * metric.compute(predictions=pred_str, references=label_str)}

    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR, 
        per_device_train_batch_size=2, 
        learning_rate=1e-5, 
        max_steps=50, # å¼ºåˆ¶æœ€å¤§æ­¥æ•°ï¼Œé¿å…å•æ ·æœ¬æ— é™è®­ç»ƒ
        fp16=torch.cuda.is_available(), 
        logging_steps=logging_steps, 
        save_steps=save_steps, 
        eval_strategy=eval_strategy, # åŠ¨æ€è°ƒæ•´éªŒè¯ç­–ç•¥
        report_to=[],
        remove_unused_columns=False
    )

    trainer = Seq2SeqTrainer(
        args=training_args, model=model, train_dataset=dataset["train"], 
        eval_dataset=dataset["test"] if eval_strategy != "no" else None,
        data_collator=DataCollator(processor), compute_metrics=compute_metrics, tokenizer=processor.feature_extractor
    )

    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    processor.save_pretrained(OUTPUT_DIR)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ–°æ¨¡å‹å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()
